### 요약: 회사 전용 LLM RAG 시스템 구축 진행 상황

**1. 데이터 추출기 구현 및 스키마 표준화 완료:**
   - Jira, Confluence Cloud, MS365 SharePoint, MS365 Teams에 대한 데이터 추출기 스크립트를 구현했습니다.
   - 각 추출기는 'cupping' Jira 프로젝트, 'o2olab' Confluence 스페이스, 'o2olab group' SharePoint 사이트, 지정된 Teams 그룹의 데이터를 대상으로 합니다.
   - 모든 추출된 데이터는 통일된 JSON Lines(JSONL) 형식의 표준화된 스키마를 따르도록 수정되었습니다. 이 스키마는 고유 ID, 원본 정보, URL, 제목, 내용, 콘텐츠 유형, 생성/업데이트 시간, 작성자 및 원본별 메타데이터를 포함합니다.

**2. 기술 스택 결정:**
   - **벡터 데이터베이스:** ChromaDB가 쉬운 사용성과 유연성을 고려하여 선정되었습니다.
   - **LLM:** OpenAI의 GPT 모델(기본값은 GPT-3.5-turbo)이 RAG 시스템의 LLM으로 선정되었습니다.
   - **개발 언어:** 모든 새로운 스크립트는 Python으로 구현되었습니다.

**3. 핵심 RAG 구성 요소 구현 완료:**
   - **데이터 로더 (`company_llm_rag/data_loader.py`):** JSONL 데이터를 읽고, 내용을 청크로 분할하고, OpenAI 임베딩 모델(text-embedding-ada-002)을 사용하여 임베딩을 생성하며, 이 데이터를 ChromaDB에 로드하는 스크립트를 개발했습니다.
   - **검색 모듈 (`company_llm_rag/retrieval_module.py`):** 사용자 쿼리에 대한 임베딩을 생성하고 ChromaDB에서 관련 문서 청크를 검색하는 기능을 구현했습니다.
   - **RAG 시스템 (`company_llm_rag/rag_system.py`):** 검색 모듈의 결과와 사용자 쿼리를 결합하여 LLM에 전달할 프롬프트를 구성하고, LLM(OpenAI GPT 모델)으로부터 답변을 생성하는 엔드-투-엔드 시스템을 개발했습니다.

**4. 배포 및 평가 계획 수립:**
   - **배포:** Docker를 이용한 컨테이너화, Kubernetes 또는 Docker Compose를 통한 오케스트레이션, 환경 변수 및 영구 저장소 관리, 확장성, 모니터링 및 로깅에 대한 계획을 수립했습니다.
   - **평가:** 데이터 추출 품질, 검색 모듈 성능(관련성, 재현율, 정밀도, 지연 시간), 종단 간 RAG 시스템 성능(답변 품질, 환각 발생률, 응답 지연 시간), 사용자 피드백을 포함한 포괄적인 평가 전략을 수립했습니다.

**다음 단계:**
   - 시스템 사용을 위한 환경 변수 설정 및 필요한 Python 패키지 설치.
   - 각 추출기를 실행하여 데이터를 추출하고, 데이터 로더를 통해 ChromaDB에 데이터를 로드.
   - `rag_system.py`를 실행하여 RAG 시스템과 상호작용.